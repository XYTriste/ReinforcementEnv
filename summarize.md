# 学习总结
## 蒙特卡洛学习
在上周的学习中，我使用蒙特卡洛学习训练了一个基于21点游戏环境的智能体。使用$\epsilon \quad greedy \quad policy$策略获得动作，不断循环得到一条完整的状态序列。并根据状态序列中各个状态的return来更新状态价值以及行为价值，这是对于状态、行为价值的无偏估计。最终的训练效果为:
> 玩家胜率约28%，不输的概率约35%，庄家胜率约65%.

然而这个训练效果并不使人满意。事实上按照经验推测，在21点这样**具有高度随机性**的游戏中（高度随机性指的是，相同状态下采取相同动作在不同的状态序列中得到的后续状态也会不同），双方的胜率应该差不多相等。显然我们对于环境的搭建、算法的认识依然存在偏差。
\
随后我尝试进行了一些调整:
1. 尝试调整$\epsilon$的取值，在探索率与当前最优的行为中做出取舍。根据我们学过的知识，探索率太高会导致收敛速度太慢，探索率过低则会导致智能体陷入局部最优解，很难去尝试在已有最优解基础上找到一个更优的解。
2. 尝试定义更完整的观测空间，观测空间我个人理解为我们从"上帝视角"能够观测到的环境与智能体的状态信息等。
## 超参数
超参数（$hyperparameters$）是在人工智能算法中需要手动指定的参数，用于控制算法的行为和性能。与模型参数不同，超参数不能从数据中学习，需要通过经验、实验或优化等方法来确定最佳值。常见的超参数包括学习率、正则化系数、批量大小、神经网络层数和节点数、损失函数等。超参数的选择对模型的性能和泛化能力有重要影响，因此超参数的选择需要谨慎。